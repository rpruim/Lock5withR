\Sexpr{set_parent('Lock5withR.Rnw')}

\setcounter{chapter}{2}
\Chapter{Confidence Intervals}

\section{Sampling Distributions}

The key idea in this chapter is the notion of a sampling distribution.  Do not confuse it with 
the population (what we would like to know about) or the sample (what we actually have data about).
If we could repeatedly sample from a population, and if we computed a statistic from each sample,
the distribution of those statistics would be the sampling distribution.  Sampling distributions
tell us how things vary from sample to sample and are the key to interpreting data.

%Example 3.1
%Example 3.2
%Example 3.3
\subsection*{Variability of Sample Statistics}

\subsubsection*{Example 3.4}
<<Example3.4>>=
head(StatisticsPhD)
mean(~FTGradEnrollment, data=StatisticsPhD) # mean enrollment in original population
@

\subsubsection*{Example 3.5}
To select a random sample of a certain size in \R, we can use the \function{sample()} function.
<<Example3.5, cache=TRUE>>=
sample10 <- sample(StatisticsPhD, 10); sample10
x.bar <- mean(~FTGradEnrollment, data=sample10); x.bar  # mean enrollment in sample10
@
Note that this sample has been assigned a name to which we can refer back to find the mean of that particular sample.

<<Example3.5b, cache=TRUE>>=
mean(~FTGradEnrollment, data=sample(StatisticsPhD, 10))  # mean enrollment in another sample 
@

\subsubsection*{Figure 3.1}

We should check that that our sample distribution has an appropriate shape:
<<Figure3.1, opts.label="fig1", cache=TRUE>>=
# Now we'll do it 1000 times
sampledist <- do(1000) * mean(~FTGradEnrollment, data=sample(StatisticsPhD, 10))
head(sampledist, 3)
dotPlot(~result, n=50, data=sampledist)
@

In many (but not all) situations, the sampling distribution is 
\begin{itemize}
  \item unimodal,
  \item symmetric, and
	\item bell-shaped
\end{itemize}
(The technical phrase is ``approximately normal''.)  In these situations, a 95\% confidence interval can be estimated 
with
\[ 
\mbox{statistic} \pm 2 SE
\]

\subsubsection*{Example 3.6}

This time we don't have data, but instead we have a summary of the data. We can however, still simulate the sample distribution by using the \function{rflip()} function.
<<Example3.6, opts.label="fig4", cache=TRUE>>=
sampledist.deg <- do(1000) * rflip(200, 0.275) # 1000 samples, each of size 200 and proportion 0.275
head(sampledist.deg, 3)
dotPlot(~ prop, width=.005, data=sampledist.deg)
@

\subsection*{Measuring Sampling Variability: The Standard Error}

\begin{boxedText}
  The standard deviation of a sampling distribution is called the \textbf{standard error}, 
	denoted $SE$.
\end{boxedText}

The standard error is our primary way of measuring how much variability there is from sample statistic
to sample statistic, and therefore how precise our estimates are.

\subsubsection*{Example 3.7}
Calculating the SE is the same as calculating the standard deviation of a sampling distribution, so we use \function{sd()}.

<<Example3.7>>=
SE <- sd(~result, data=sampledist); SE    # sample from Example 3.5
SE2 <- sd(~prop, data=sampledist.deg); SE2     # sample from Example 3.6
@

%Example 3.8

\subsection*{The Importance of Sample Size}

\subsubsection*{Example 3.9}

<<Example3.9>>=
sampledist.1000 <- do(1000) * rflip(1000, 0.275) # 1000 samples, each of size 1000 and proportion 0.275
sampledist.200 <- do(1000) * rflip(200, 0.275)   # 1000 samples, each of size 200 and proportion 0.275
sampledist.50 <- do(1000) * rflip(50, 0.275)     # 1000 samples, each of size 50 and proportion 0.275
@

\subsubsection*{Figure 3.3}

<<Figure3.3, opts.label="fig3">>=
dotPlot(~ prop, width=.005, xlim=c(0.05, 0.5), data=sampledist.1000)
dotPlot(~ prop, width=.005, xlim=c(0.05, 0.5), data=sampledist.200)
dotPlot(~ prop, width=.005, xlim=c(0.05, 0.5), data=sampledist.50)
@

%Example 3.10
%Example 3.11

\section{Understanding and Interpreting Confidence Intervals}

\subsection*{Interval Estimates and Margin of Error}

\begin{boxedText}
An \term{interval estimate} 
gives a range of plausible values for a population parameter.  
\end{boxedText}

This is better than a single number (also called a point estimate) because it gives some 
indication of the precision of the estimate.

One way to express an interval estimate is with a point estimate and a \term{margin of error}.

We can convert margin of error into an interval by adding and subtracting the margin of error to/from
the statistic.

\subsubsection*{Example 3.12}

<<Example3.12, tidy=FALSE>>=
p.hat <- 0.42                       # sample proportion
MoE <- 0.03                         # margin of error
p.hat - MoE                         # lower limit of interval estimate
p.hat + MoE                         # upper limit of interval estimate
@

\subsubsection*{Example 3.13}

<<Example3.13, tidy=FALSE>>=
p.hat <- 0.54                       # sample proportion
MoE <- 0.02                         # margin of error
p.hat - MoE                         # lower limit of interval estimate
p.hat + MoE                         # upper limit of interval estimate
@

<<Example3.13b>>=
p.hat <- 0.54 
MoE <- 0.10
p.hat - MoE
p.hat + MoE
@

\subsection*{Confidence Intervals}

\begin{boxedText}
A confidence interval for a parameter is an interval computed from sample data
by a method that will capture the parameter for a specified proportion of all
samples
\end{boxedText}

\begin{enumerate}
	\item The probability of correctly containing the parameter is called the coverage rate or \term{confidence level}.
	\item So 95\% of 95\% confidence intervals contain the parameter being estimated.
	\item The margins of error in the tables above were designed to produce 95\% confidence intervals.
\end{enumerate}

\subsubsection*{Example 3.14}

<<Example3.14, tidy=FALSE>>=
x.bar <- 61.5              # given sample mean
SE <- 11                   # given estimated standard error
MoE <- 2 * SE; MoE         # margin of error for 95% CI
x.bar - MoE                # lower limit of 95% CI
x.bar + MoE                # upper limit of 95% CI
@

\subsection*{Understanding Confidence Intervals}
\subsubsection*{Example 3.15}

<<Example3.15>>=
SE <- 0.03
p1 <- 0.26 
p2 <- 0.32
p3 <- 0.20
MoE <- 2 * SE
@

<<Example3.15b>>=
p1-MoE
p1+MoE
p2-MoE
p2+MoE
p3-MoE
p3+MoE
@

\subsubsection*{Figure 3.12}

<<Figure3.12, opts.label="fig4">>=
p <- 0.275
SE <- 0.03
MoE <- 2 * SE
p - MoE
p + MoE
dotPlot(~ prop, width=.005, groups = (0.215 <= prop & prop <= 0.335), data=sampledist.deg)
@

\subsubsection*{Figure 3.13}
We can create the data needed for plots like Figure 3.13 using \function{CIsim()}.  The plot
itself uses \function{xYplot()} from the \pkg{Hmisc} package.
<<Figure3.13, message=FALSE, seed=1234, opts.label="fig1">>=
results <- CIsim(200, samples=3, rdist=rbinom, args=list(size=1, prob=0.275), 
                 method=binom.test, method.args=list(success=1), verbose=FALSE, estimand=0.275)
require(Hmisc)
xYplot(Cbind(estimate,lower,upper) ~ sample,
  data=results,
  par.settings=col.mosaic(), 
  groups=cover)
@

<<Figure3.13b, message=FALSE, seed=1234, opts.label="fig1">>=
results <- CIsim(200, samples=100, rdist=rbinom, args=list(size=1, prob=0.275), 
                 method=binom.test, method.args=list(success=1), verbose=FALSE, estimand=0.275)
require(Hmisc)
xYplot(Cbind(estimate,lower,upper) ~ sample,
  data=results,
  par.settings=col.mosaic(), 
  groups=cover)
@

\subsection*{Interpreting Confidence Intervals}

\subsubsection*{Example 3.16}

<<Example3.16>>=
x.bar <- 27.655
SE <- 0.009
MoE <- 2 * SE
x.bar - MoE
x.bar + MoE
@

\subsubsection*{Example 3.17}

<<Example3.17>>=
diff.x <- -1.915
SE <- 0.016
MoE <- 2 * SE
diff.x - MoE
diff.x + MoE
@

%Example 3.18

\section{Constructing Bootstrap Confidence Intervals}

Here's the clever idea:  We don't have the population, but we have a sample.  Probably the sample it similar to the population in many ways.  So let's sample from our sample.  We'll call it \textbf{resampling} (also called \textbf{bootstrapping}). We want samples the same size as our original sample, so we will need to sample with replacement.  This means that we may pick some members of the population more than once and others not at all.  We'll do this many times, however, so each member of our sample will get its fair share. (Notice the similarity to and difference from sampling from populations in the previous sections.) 

\subsubsection*{Figure 3.14}
<<Figure3.14, opts.lable="fig1">>=
dotPlot(~Time, width=1, cex=.5, data=CommuteAtlanta)
@

\subsection*{Bootstrap Samples}
The computer can easily do all of the resampling by using the \function{resample()}.
<<Table3.7>>=
mean(~Time, data = resample(CommuteAtlanta)) # mean commute time in one resample
mean(~Time, data = resample(CommuteAtlanta)) # mean commute time in another resample
mean(~Time, data = resample(CommuteAtlanta))
@

\subsection*{Bootstrap Distribution}

The example below uses data from 500 Atlanta commuters.
<<Figure3.16, cache=TRUE, opts.lable="fig1">>=
# Now we'll do it 1000 times
Bootstrap <- do(1000) * mean( ~Time, data=resample(CommuteAtlanta))
head(Bootstrap, 3)
# We should check that that our bootstrap distribution has an appropriate shape:
dotPlot(~result, n=50, data=Bootstrap)
@

\subsubsection*{Example 3.19}
<<Example3.19, cache=TRUE, opts.label="fig4">>=
BootP <- do(1000) * rflip(100, .52)
head(BootP, 3)
dotPlot(~ prop, width=.01, data=BootP)
@

\subsubsection*{Example 3.20}
Variables can be created in \R\ using the \function{c()} function then collected into
a data frame using the \function{data.frame()} function.

<<Example3.20>>=
Laughter <- data.frame( NumLaughs =  c(16, 22, 9, 31, 6, 42) )
mean( ~ NumLaughs, data=Laughter )
@

<<Example3.20b>>=
mean( ~NumLaughs, data=resample(Laughter))
mean( ~NumLaughs, data=resample(Laughter))
mean( ~NumLaughs, data=resample(Laughter))
@

\subsection*{Estimating Standard Error Based on a Bootstrap Distribution}

\subsubsection*{Example 3.21}
Since the shape of the bootstrap distribution from Example 3.19 looks good, we can estimate the standard error.
<<Example3.21>>=
SE <- sd(~prop, data=BootP); SE
@

\subsection*{95 \% Confidence Interval Based on a Bootstrap Standard Error}

\subsubsection*{Example 3.22}
We can again use the standard error to compute a 95\% confidence interval.
<<Example3.22, tidy=FALSE>>=
x.bar <- mean(~Time, data = CommuteAtlanta); x.bar
SE <- sd(~result, data = Bootstrap ); SE        # standard error
MoE <- 2 * SE; MoE                              # margin of error for 95% CI
x.bar - MoE                                     # lower limit of 95% CI
x.bar + MoE                                     # upper limit of 95% CI
@

<<Example3.22b>>=
p.hat = 0.52
SE = sd( ~prop, data=BootP); SE      
MoE = 2 * SE; MoE                          
p.hat - MoE                                 
p.hat + MoE                                 
@
The same steps used in this example, get used in a wide variety of confidence interval situations.
\begin{enumerate}
  \item 
		Compute the statistic from the original sample.
	\item
		Create a bootstrap distribution by resampling from the sample.
		\begin{enumerate}
			\item same size samples as the original sample
			\item with replacement
			\item compute the statistic for each sample
		\end{enumerate}
		The distribution of these statistics is the bootstrap distribution
	\item
		Estimate the standard error $SE$ by computing the standard deviation of the bootstrap distribution.
	\item
		95\% CI is \[ \mbox{statistic} \pm 2 SE \]
\end{enumerate}


\section{Bootstrap Confidence Intervals Using Percentiles}

\subsection*{Confidence Intervals Based on Bootstrap Percentiles}

\subsubsection*{Example 3.23}

Another way to create a 95\% confidence interval is to use the middle 95\% of the bootstrap distribution.
The \function{cdata()} function can compute this for us as follows:
<<Example3.23>>=
cdata(0.95, result, data=Bootstrap)
@
This is not exactly the same as the interval of the original sample, but it is pretty close.

\subsubsection*{Figure 3.22}

<<Figure3.22, opts.label="fig1">>=
dotPlot(~ result, width=.2, groups = (27.47 <= result & result <= 31.00), data=Bootstrap)
@
Notice the \argument{groups=} for marking the confidence interval.

\subsubsection*{Example 3.24}

One advantage of this method is that it is easy to change the confidence level. 

To make a 90\% confidence interval, we use the middle 90\% of the sample distribution instead.
<<Example3.24, opts.label="fig1">>=
cdata(0.99, result, data=Bootstrap)
dotPlot(~ result, width=.2, groups = (26.79 <= result & result <= 31.46), data=Bootstrap)

cdata(0.90, result, data=Bootstrap)
dotPlot(~ result, width=.2, groups = (27.64 <= result & result <= 30.55), data=Bootstrap)
@
Notice that this interval is narrower.  This will always be the case.  Higher levels of confidence 
lead to wider confidence intervals.

\subsection*{Finding Confidence Intervals for Many Different Parameters}

\subsubsection*{Figure 3.24}

<<Figure3.25>>=
bwplot(Gender~Exercise, data=ExerciseHours)
@

\subsubsection*{Example 3.25}

<<Example3.25>>=
head(ExerciseHours)
favstats(~Exercise|Gender, data=ExerciseHours)
stat <- diff(mean(Exercise~Gender, data=ExerciseHours)); stat
@
<<Example3.25b, cache=TRUE>>=
BootE <- do(3000) * diff(mean(Exercise~Gender, data=resample(ExerciseHours)))
head(BootE, 3)
@
<<Example3.25copts.label="fig4">>=
cdata(0.95, M, data=BootE)
dotPlot(~M, width=.25, cex=.5, groups =(-1.44 <= M & M <= 7.534), 
        xlab="Difference in mean", data=BootE)
@
<<Example3.25d>>=
SE <- sd( ~ M, data=BootE); SE
stat - 2 * SE
stat + 2 * SE
@

\subsubsection*{Figure 3.26}

<<Figure3.26>>=
xyplot(Price~Miles, ylab="Price ($1000s)", xlab="Miles (1000s)", data=MustangPrice)
cor(Price~Miles, data=MustangPrice)
@

\subsubsection*{Example 3.26}

<<Example3.26, cache=TRUE>>=
BootM <- do(5000) * cor(Price~Miles, data=resample((MustangPrice)))
head(BootM, 3)
@
<<Example3.26b, opts.label="fig4">>=
cdata(0.98, result, data=BootM)
dotPlot(~result, width=.005, groups =(-.9394 <= result & result <= -.7060), xlab="r", data=BootM)
@

\subsection*{Another Look at the Effect of Sample Size}

\subsubsection*{Example 3.27}
<<Example3.27, opts.label="fig4", cache=TRUE>>=
BootP400 <- do(1000) * rflip(400, .52)
head(BootP400, 3)
cdata(0.95, prop, data=BootP400)
dotPlot(~ prop, width=.005, groups= (0.47 <= prop & prop<= 0.5675), data=BootP400)
@

\subsection*{One Caution on Constructing Bootstrap Confidence Intervals}

\subsubsection*{Example 3.28}
<<Example3.28, opts.label="fig4">>=
median(~ Price, data=MustangPrice )
Boot.Mustang <- do(5000) * median( ~Price, data=resample(MustangPrice) )
head(Boot.Mustang, 3)
histogram( ~ result, n=50, data=Boot.Mustang)
@

This time the histogram does not have the desired shape.  There are two problems:
\begin{enumerate}
  \item
		The distribution is not symmetric.  (It is right skewed.)
	\item
		The distribution has spikes and gaps.

Since the median must be
an element of the sample when the sample size is 25, there are only 25 possible values for the median (and some of 
these are \emph{very} unlikely.
\end{enumerate}
Since the bootstrap distribution does not look like a normal distribution (bell-shaped, symmetric), we cannot safely use 
our methods for creating a confidence interval.